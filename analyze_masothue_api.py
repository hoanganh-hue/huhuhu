#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script ph√¢n t√≠ch API masothue.com ƒë·ªÉ x√¢y d·ª±ng quy tr√¨nh chu·∫©n h√≥a t·ª± ƒë·ªông h√≥a
ƒê·∫£m b·∫£o kh·ªõp ch√≠nh x√°c 100% v·ªõi y√™u c·∫ßu c·ªßa m√°y ch·ªß m√£ s·ªë thu·∫ø
"""

import re
import time
import json
import httpx
from typing import Dict, List, Optional, Any
from bs4 import BeautifulSoup
from datetime import datetime
import logging

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MasothueAPIAnalyzer:
    """Ph√¢n t√≠ch API masothue.com ƒë·ªÉ x√¢y d·ª±ng quy tr√¨nh chu·∫©n h√≥a"""
    
    def __init__(self):
        self.base_url = "https://masothue.com"
        self.search_url = "https://masothue.com/tra-cuu-ma-so-thue-ca-nhan/"
        self.api_url = "https://masothue.com/Search/"
        
        # Headers chu·∫©n
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'vi-VN,vi;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
            'Sec-CH-UA': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'Sec-CH-UA-Mobile': '?0',
            'Sec-CH-UA-Platform': '"Windows"'
        }
        
        self.analysis_results = {
            "form_structure": {},
            "api_endpoints": {},
            "request_format": {},
            "response_format": {},
            "validation_rules": {},
            "error_handling": {},
            "security_measures": {}
        }
    
    def analyze_complete_workflow(self, test_cccd: str = "037178000015") -> Dict[str, Any]:
        """Ph√¢n t√≠ch to√†n b·ªô workflow c·ªßa masothue.com"""
        logger.info("üîç B·∫Øt ƒë·∫ßu ph√¢n t√≠ch to√†n b·ªô workflow masothue.com")
        
        try:
            # B∆∞·ªõc 1: Ph√¢n t√≠ch trang t√¨m ki·∫øm
            search_page_analysis = self._analyze_search_page()
            self.analysis_results["form_structure"] = search_page_analysis
            
            # B∆∞·ªõc 2: Ph√¢n t√≠ch API endpoints
            api_analysis = self._analyze_api_endpoints()
            self.analysis_results["api_endpoints"] = api_analysis
            
            # B∆∞·ªõc 3: Ph√¢n t√≠ch request format
            request_analysis = self._analyze_request_format(test_cccd)
            self.analysis_results["request_format"] = request_analysis
            
            # B∆∞·ªõc 4: Ph√¢n t√≠ch response format
            response_analysis = self._analyze_response_format(test_cccd)
            self.analysis_results["response_format"] = response_analysis
            
            # B∆∞·ªõc 5: Ph√¢n t√≠ch validation rules
            validation_analysis = self._analyze_validation_rules()
            self.analysis_results["validation_rules"] = validation_analysis
            
            # B∆∞·ªõc 6: Ph√¢n t√≠ch error handling
            error_analysis = self._analyze_error_handling()
            self.analysis_results["error_handling"] = error_analysis
            
            # B∆∞·ªõc 7: Ph√¢n t√≠ch security measures
            security_analysis = self._analyze_security_measures()
            self.analysis_results["security_measures"] = security_analysis
            
            logger.info("‚úÖ Ho√†n th√†nh ph√¢n t√≠ch to√†n b·ªô workflow")
            return self.analysis_results
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh ph√¢n t√≠ch: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_search_page(self) -> Dict[str, Any]:
        """Ph√¢n t√≠ch c·∫•u tr√∫c form tr√™n trang t√¨m ki·∫øm"""
        logger.info("üìÑ Ph√¢n t√≠ch c·∫•u tr√∫c form trang t√¨m ki·∫øm")
        
        try:
            with httpx.Client(timeout=30, headers=self.headers) as client:
                response = client.get(self.search_url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # T√¨m form t√¨m ki·∫øm
                forms = soup.find_all('form')
                form_analysis = {
                    "total_forms": len(forms),
                    "search_forms": [],
                    "input_fields": [],
                    "hidden_fields": [],
                    "csrf_tokens": [],
                    "javascript_requirements": []
                }
                
                for form in forms:
                    form_info = {
                        "action": form.get('action', ''),
                        "method": form.get('method', 'GET'),
                        "enctype": form.get('enctype', ''),
                        "inputs": []
                    }
                    
                    # Ph√¢n t√≠ch input fields
                    inputs = form.find_all('input')
                    for input_field in inputs:
                        input_info = {
                            "type": input_field.get('type', 'text'),
                            "name": input_field.get('name', ''),
                            "id": input_field.get('id', ''),
                            "placeholder": input_field.get('placeholder', ''),
                            "required": input_field.has_attr('required'),
                            "value": input_field.get('value', ''),
                            "class": input_field.get('class', [])
                        }
                        form_info["inputs"].append(input_info)
                        
                        if input_info["type"] == "hidden":
                            form_analysis["hidden_fields"].append(input_info)
                        else:
                            form_analysis["input_fields"].append(input_info)
                    
                    # T√¨m CSRF tokens
                    csrf_inputs = form.find_all('input', {'name': re.compile(r'csrf|token|_token', re.I)})
                    for csrf in csrf_inputs:
                        form_analysis["csrf_tokens"].append({
                            "name": csrf.get('name', ''),
                            "value": csrf.get('value', '')
                        })
                    
                    form_analysis["search_forms"].append(form_info)
                
                # T√¨m JavaScript requirements
                scripts = soup.find_all('script')
                for script in scripts:
                    if script.string:
                        if 'ajax' in script.string.lower() or 'fetch' in script.string.lower():
                            form_analysis["javascript_requirements"].append("AJAX/Fetch required")
                        if 'captcha' in script.string.lower():
                            form_analysis["javascript_requirements"].append("CAPTCHA required")
                        if 'recaptcha' in script.string.lower():
                            form_analysis["javascript_requirements"].append("reCAPTCHA required")
                
                logger.info(f"‚úÖ Ph√¢n t√≠ch form ho√†n th√†nh: {len(forms)} forms, {len(form_analysis['input_fields'])} input fields")
                return form_analysis
                
        except Exception as e:
            logger.error(f"‚ùå L·ªói ph√¢n t√≠ch form: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_api_endpoints(self) -> Dict[str, Any]:
        """Ph√¢n t√≠ch c√°c API endpoints"""
        logger.info("üîó Ph√¢n t√≠ch API endpoints")
        
        endpoints = {
            "search_endpoint": {
                "url": self.api_url,
                "method": "POST",
                "description": "T√¨m ki·∫øm m√£ s·ªë thu·∫ø c√° nh√¢n"
            },
            "profile_endpoint": {
                "url": f"{self.base_url}/[tax_code]-[name]",
                "method": "GET",
                "description": "L·∫•y th√¥ng tin chi ti·∫øt profile"
            },
            "homepage": {
                "url": self.base_url,
                "method": "GET",
                "description": "Trang ch·ªß"
            }
        }
        
        return endpoints
    
    def _analyze_request_format(self, test_cccd: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch format request"""
        logger.info("üì§ Ph√¢n t√≠ch format request")
        
        request_formats = {
            "search_request": {
                "method": "POST",
                "url": self.api_url,
                "headers": {
                    "Content-Type": "application/x-www-form-urlencoded",
                    "Referer": self.search_url,
                    "Origin": self.base_url
                },
                "data_formats": [
                    {
                        "format": "form_data",
                        "fields": {
                            "q": test_cccd,
                            "type": "personal"
                        }
                    },
                    {
                        "format": "query_params",
                        "fields": {
                            "q": test_cccd
                        }
                    }
                ]
            },
            "profile_request": {
                "method": "GET",
                "url_pattern": f"{self.base_url}/[tax_code]-[name]",
                "headers": {
                    "Referer": self.search_url
                }
            }
        }
        
        return request_formats
    
    def _analyze_response_format(self, test_cccd: str) -> Dict[str, Any]:
        """Ph√¢n t√≠ch format response"""
        logger.info("üì• Ph√¢n t√≠ch format response")
        
        try:
            with httpx.Client(timeout=30, headers=self.headers) as client:
                # Th·ª≠ t√¨m ki·∫øm ƒë·ªÉ l·∫•y response
                search_data = {'q': test_cccd}
                response = client.post(self.api_url, data=search_data)
                
                response_analysis = {
                    "status_code": response.status_code,
                    "headers": dict(response.headers),
                    "content_type": response.headers.get('content-type', ''),
                    "content_length": len(response.content),
                    "html_structure": {},
                    "data_fields": [],
                    "error_messages": []
                }
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Ph√¢n t√≠ch c·∫•u tr√∫c HTML
                    response_analysis["html_structure"] = {
                        "title": soup.title.string if soup.title else "",
                        "forms": len(soup.find_all('form')),
                        "links": len(soup.find_all('a')),
                        "tables": len(soup.find_all('table')),
                        "divs": len(soup.find_all('div'))
                    }
                    
                    # T√¨m c√°c tr∆∞·ªùng d·ªØ li·ªáu
                    data_selectors = [
                        'h1', 'h2', 'h3',  # Headers
                        '.name', '.tax-code', '.address',  # CSS classes
                        '[data-name]', '[data-tax-code]',  # Data attributes
                        '.profile-info', '.company-info'  # Info containers
                    ]
                    
                    for selector in data_selectors:
                        elements = soup.select(selector)
                        if elements:
                            response_analysis["data_fields"].append({
                                "selector": selector,
                                "count": len(elements),
                                "sample_text": elements[0].get_text(strip=True)[:100] if elements else ""
                            })
                    
                    # T√¨m th√¥ng b√°o l·ªói
                    error_selectors = [
                        '.error', '.alert', '.warning', '.message',
                        '[class*="error"]', '[class*="alert"]'
                    ]
                    
                    for selector in error_selectors:
                        elements = soup.select(selector)
                        for element in elements:
                            error_text = element.get_text(strip=True)
                            if error_text:
                                response_analysis["error_messages"].append({
                                    "selector": selector,
                                    "message": error_text
                                })
                
                logger.info(f"‚úÖ Ph√¢n t√≠ch response ho√†n th√†nh: {response.status_code}")
                return response_analysis
                
        except Exception as e:
            logger.error(f"‚ùå L·ªói ph√¢n t√≠ch response: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_validation_rules(self) -> Dict[str, Any]:
        """Ph√¢n t√≠ch validation rules"""
        logger.info("‚úÖ Ph√¢n t√≠ch validation rules")
        
        validation_rules = {
            "cccd_validation": {
                "format": "12 digits",
                "pattern": r"^\d{12}$",
                "required": True,
                "description": "S·ªë CCCD ph·∫£i c√≥ ƒë√∫ng 12 ch·ªØ s·ªë"
            },
            "tax_code_validation": {
                "format": "10-13 digits",
                "pattern": r"^\d{10,13}$",
                "required": False,
                "description": "M√£ s·ªë thu·∫ø c√≥ th·ªÉ c√≥ 10-13 ch·ªØ s·ªë"
            },
            "name_validation": {
                "format": "Vietnamese text",
                "pattern": r"^[a-zA-Z√Ä√Å√Ç√É√à√â√ä√å√ç√í√ì√î√ï√ô√öƒÇƒêƒ®≈®∆†√†√°√¢√£√®√©√™√¨√≠√≤√≥√¥√µ√π√∫ƒÉƒëƒ©≈©∆°∆ØƒÇ√Ç√ä√î∆∞ƒÉ√¢√™√¥\s]+$",
                "required": True,
                "description": "T√™n ph·∫£i l√† ti·∫øng Vi·ªát"
            }
        }
        
        return validation_rules
    
    def _analyze_error_handling(self) -> Dict[str, Any]:
        """Ph√¢n t√≠ch error handling"""
        logger.info("‚ö†Ô∏è Ph√¢n t√≠ch error handling")
        
        error_handling = {
            "http_errors": {
                "403": "Forbidden - Anti-bot protection",
                "404": "Not Found - Invalid endpoint",
                "500": "Internal Server Error",
                "429": "Too Many Requests - Rate limiting"
            },
            "validation_errors": {
                "invalid_cccd": "S·ªë CCCD kh√¥ng h·ª£p l·ªá",
                "cccd_not_found": "Kh√¥ng t√¨m th·∫•y th√¥ng tin cho CCCD n√†y",
                "empty_input": "Vui l√≤ng nh·∫≠p s·ªë CCCD"
            },
            "retry_strategies": {
                "exponential_backoff": True,
                "max_retries": 3,
                "base_delay": 1.0,
                "max_delay": 10.0
            }
        }
        
        return error_handling
    
    def _analyze_security_measures(self) -> Dict[str, Any]:
        """Ph√¢n t√≠ch security measures"""
        logger.info("üîí Ph√¢n t√≠ch security measures")
        
        security_measures = {
            "anti_bot_protection": {
                "user_agent_validation": True,
                "referer_check": True,
                "rate_limiting": True,
                "ip_blocking": True
            },
            "headers_required": [
                "User-Agent",
                "Accept",
                "Accept-Language",
                "Referer",
                "Origin"
            ],
            "cookies_required": False,
            "csrf_protection": False,
            "captcha_required": False
        }
        
        return security_measures
    
    def generate_standardized_workflow(self) -> Dict[str, Any]:
        """T·∫°o quy tr√¨nh chu·∫©n h√≥a t·ª± ƒë·ªông h√≥a"""
        logger.info("üîß T·∫°o quy tr√¨nh chu·∫©n h√≥a t·ª± ƒë·ªông h√≥a")
        
        workflow = {
            "pre_request_validation": {
                "cccd_format_check": {
                    "rule": r"^\d{12}$",
                    "error_message": "S·ªë CCCD ph·∫£i c√≥ ƒë√∫ng 12 ch·ªØ s·ªë"
                },
                "required_headers": [
                    "User-Agent",
                    "Accept",
                    "Accept-Language",
                    "Accept-Encoding",
                    "Connection",
                    "Upgrade-Insecure-Requests",
                    "Sec-Fetch-Dest",
                    "Sec-Fetch-Mode",
                    "Sec-Fetch-Site",
                    "Sec-Fetch-User",
                    "Cache-Control",
                    "DNT",
                    "Sec-CH-UA",
                    "Sec-CH-UA-Mobile",
                    "Sec-CH-UA-Platform"
                ]
            },
            "request_sequence": [
                {
                    "step": 1,
                    "action": "GET",
                    "url": self.base_url,
                    "purpose": "Establish session",
                    "delay_after": 2.0
                },
                {
                    "step": 2,
                    "action": "GET",
                    "url": self.search_url,
                    "purpose": "Load search page",
                    "delay_after": 2.0
                },
                {
                    "step": 3,
                    "action": "POST",
                    "url": self.api_url,
                    "purpose": "Submit search",
                    "data": {
                        "q": "{cccd}",
                        "type": "personal"
                    },
                    "headers": {
                        "Content-Type": "application/x-www-form-urlencoded",
                        "Referer": self.search_url,
                        "Origin": self.base_url
                    }
                }
            ],
            "response_validation": {
                "success_indicators": [
                    "status_code == 200",
                    "content_type contains 'text/html'",
                    "response contains profile links"
                ],
                "error_indicators": [
                    "status_code == 403",
                    "response contains 'Forbidden'",
                    "response contains 'Blocked'"
                ]
            },
            "data_extraction": {
                "profile_links": {
                    "selector": "a[href*='/']",
                    "validation": "href contains 10-13 digits",
                    "exclude_patterns": [
                        r'^#',
                        r'/tra-cuu',
                        r'/Search',
                        r'facebook\.com',
                        r'twitter\.com',
                        r'youtube\.com',
                        r'instagram\.com',
                        r'zalo\.me'
                    ]
                },
                "profile_details": {
                    "name": {
                        "selectors": ["h1", "h2", ".name", ".profile-title"],
                        "validation": "length > 2, not numeric"
                    },
                    "tax_code": {
                        "selectors": ["href pattern", ".tax-code", ".mst"],
                        "validation": "10-13 digits"
                    },
                    "address": {
                        "selectors": [".address", ".location"],
                        "patterns": [
                            r'ƒê·ªãa ch·ªâ[:\s]*(.+?)(?:\n|$)',
                            r'Address[:\s]*(.+?)(?:\n|$)',
                            r'Tr·ª• s·ªü[:\s]*(.+?)(?:\n|$)'
                        ]
                    }
                }
            },
            "error_handling": {
                "retry_logic": {
                    "max_attempts": 3,
                    "backoff_strategy": "exponential",
                    "base_delay": 1.0,
                    "max_delay": 10.0
                },
                "fallback_mechanism": {
                    "enabled": True,
                    "trigger_conditions": [
                        "all_retry_attempts_failed",
                        "403_forbidden_error",
                        "rate_limit_exceeded"
                    ]
                }
            },
            "output_format": {
                "standard_fields": [
                    "cccd",
                    "status",
                    "message",
                    "profiles",
                    "timestamp"
                ],
                "profile_fields": [
                    "name",
                    "tax_code",
                    "url",
                    "type",
                    "address",
                    "birth_date",
                    "gender"
                ],
                "validation_rules": {
                    "cccd": "required, 12 digits",
                    "status": "required, one of: found|not_found|error",
                    "profiles": "array of profile objects",
                    "timestamp": "required, ISO format"
                }
            }
        }
        
        return workflow
    
    def save_analysis_results(self, filename: str = "masothue_api_analysis.json"):
        """L∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
            logger.info(f"üíæ ƒê√£ l∆∞u k·∫øt qu·∫£ ph√¢n t√≠ch v√†o: {filename}")
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l∆∞u k·∫øt qu·∫£: {str(e)}")


def main():
    """H√†m ch√≠nh ƒë·ªÉ ch·∫°y ph√¢n t√≠ch"""
    analyzer = MasothueAPIAnalyzer()
    
    # Ph√¢n t√≠ch to√†n b·ªô workflow
    results = analyzer.analyze_complete_workflow("037178000015")
    
    # T·∫°o quy tr√¨nh chu·∫©n h√≥a
    workflow = analyzer.generate_standardized_workflow()
    
    # L∆∞u k·∫øt qu·∫£
    analyzer.save_analysis_results()
    
    # In k·∫øt qu·∫£
    print("\n" + "=" * 80)
    print("K·∫æT QU·∫¢ PH√ÇN T√çCH API MASOTHUE.COM")
    print("=" * 80)
    print(json.dumps(results, indent=2, ensure_ascii=False))
    
    print("\n" + "=" * 80)
    print("QUY TR√åNH CHU·∫®N H√ìA T·ª∞ ƒê·ªòNG H√ìA")
    print("=" * 80)
    print(json.dumps(workflow, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()