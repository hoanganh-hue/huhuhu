#!/usr/bin/env python3
"""
Test CCCD 031089011929 using Module 2 enhanced anti-bot capabilities
Collect all displayed data fields from masothue.com
"""

import requests
import time
import logging
from bs4 import BeautifulSoup
import re
from typing import Dict, Any, Optional

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CCCDChecker:
    """Enhanced CCCD checker with anti-bot capabilities"""
    
    def __init__(self):
        # SOCKS5 proxy configuration
        self.proxy_config = {
            'http': 'socks5://beba111:tDV5tkMchYUBMD@ip.mproxy.vn:12301',
            'https': 'socks5://beba111:tDV5tkMchYUBMD@ip.mproxy.vn:12301'
        }
        
        # Browser-like headers
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
            "Accept-Language": "vi,en-US;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Sec-Fetch-User": "?1",
            "Cache-Control": "max-age=0"
        }
    
    def check_cccd(self, cccd: str) -> Dict[str, Any]:
        """Check CCCD and collect all data fields"""
        logger.info(f"ğŸ” Checking CCCD: {cccd}")
        logger.info("="*60)
        
        # Create session
        session = requests.Session()
        session.proxies.update(self.proxy_config)
        session.headers.update(self.headers)
        
        try:
            # Step 1: Get homepage to establish session
            logger.info("ğŸª Getting homepage to establish session...")
            homepage_response = session.get("https://masothue.com/", timeout=15)
            logger.info(f"âœ… Homepage status: {homepage_response.status_code}")
            
            # Step 2: Perform search
            logger.info(f"ğŸ” Searching for CCCD: {cccd}")
            search_url = f"https://masothue.com/Search/?q={cccd}&type=auto&token=NbnmgilFfL&force-search=1"
            
            # Add search-specific headers
            search_headers = self.headers.copy()
            search_headers.update({
                "Referer": "https://masothue.com/",
                "X-Requested-With": "XMLHttpRequest"
            })
            
            start_time = time.time()
            response = session.get(search_url, headers=search_headers, timeout=15)
            response_time = time.time() - start_time
            
            logger.info(f"âœ… Search status: {response.status_code}")
            logger.info(f"â±ï¸ Response time: {response_time:.2f}s")
            logger.info(f"ğŸ“Š Content length: {len(response.content)} bytes")
            
            if response.status_code == 200:
                # Parse and extract all data
                result = self._extract_all_data(response.text, cccd)
                result["response_time"] = response_time
                result["status"] = "success"
                return result
            else:
                return {
                    "cccd": cccd,
                    "status": "error",
                    "error": f"HTTP {response.status_code}",
                    "response_time": response_time
                }
                
        except Exception as e:
            logger.error(f"âŒ Error: {e}")
            return {
                "cccd": cccd,
                "status": "error",
                "error": str(e)
            }
    
    def _extract_all_data(self, html_content: str, cccd: str) -> Dict[str, Any]:
        """Extract all possible data fields from HTML content"""
        logger.info("ğŸ“„ Parsing HTML content...")
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Initialize result structure
        result = {
            "cccd": cccd,
            "tax_code": None,
            "name": None,
            "full_name": None,
            "address": None,
            "province": None,
            "district": None,
            "ward": None,
            "business_type": None,
            "business_status": None,
            "registration_date": None,
            "profile_url": None,
            "additional_info": {},
            "all_links": [],
            "all_text_content": "",
            "raw_html": html_content
        }
        
        # Extract all text content
        all_text = soup.get_text()
        result["all_text_content"] = all_text
        logger.info(f"ğŸ“„ Total text content length: {len(all_text)} characters")
        
        # Extract all links
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            if href and text:
                result["all_links"].append({
                    "url": href,
                    "text": text
                })
        
        logger.info(f"ğŸ”— Found {len(result['all_links'])} links")
        
        # Method 1: Look for tax code and name in links
        for link in all_links:
            href = link.get('href', '')
            text = link.get_text(strip=True)
            
            # Check for tax code profile links
            if '/masothue.com/' in href and href != 'https://masothue.com/':
                # Extract tax code from URL
                url_parts = href.split('/')
                if len(url_parts) > 0:
                    last_part = url_parts[-1]
                    if '-' in last_part:
                        potential_tax_code = last_part.split('-')[0]
                        if potential_tax_code.isdigit() and len(potential_tax_code) == 10:
                            result["tax_code"] = potential_tax_code
                            result["name"] = text
                            result["profile_url"] = href
                            logger.info(f"ğŸ¯ Found tax code: {potential_tax_code}")
                            logger.info(f"ğŸ‘¤ Found name: {text}")
                            logger.info(f"ğŸ”— Profile URL: {href}")
                            break
        
        # Method 2: Look for tax code in text content
        if not result["tax_code"]:
            tax_pattern = r'\b\d{10}\b'
            matches = re.findall(tax_pattern, all_text)
            if matches:
                result["tax_code"] = matches[0]
                logger.info(f"ğŸ¯ Found tax code in text: {matches[0]}")
        
        # Method 3: Look for Vietnamese names
        if not result["name"]:
            # Vietnamese name patterns
            vietnamese_patterns = [
                r'[A-ZÃ€Ãáº áº¢ÃƒÃ‚áº¦áº¤áº¬áº¨áºªÄ‚áº°áº®áº¶áº²áº´ÃˆÃ‰áº¸áººáº¼ÃŠá»€áº¾á»†á»‚á»„ÃŒÃá»Šá»ˆÄ¨Ã’Ã“á»Œá»Ã•Ã”á»’á»á»˜á»”á»–Æ á»œá»šá»¢á»á» Ã™Ãšá»¤á»¦Å¨Æ¯á»ªá»¨á»°á»¬á»®á»²Ãá»´á»¶á»¸Ä][a-zÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘\s]+',
                r'[A-Z][a-z]+\s+[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*'
            ]
            
            for pattern in vietnamese_patterns:
                matches = re.findall(pattern, all_text)
                for match in matches:
                    match = match.strip()
                    if len(match) > 5 and len(match) < 50:
                        # Check if it looks like a Vietnamese name
                        if any(char in match for char in ['LÃª', 'Nguyá»…n', 'Tráº§n', 'Pháº¡m', 'HoÃ ng', 'Phan', 'VÅ©', 'VÃµ', 'Äá»—', 'BÃ¹i', 'Äáº·ng', 'NgÃ´', 'DÆ°Æ¡ng', 'LÃ½']):
                            result["name"] = match
                            logger.info(f"ğŸ‘¤ Found Vietnamese name: {match}")
                            break
                if result["name"]:
                    break
        
        # Method 4: Look for address information
        address_keywords = ['phÆ°á»ng', 'quáº­n', 'huyá»‡n', 'tá»‰nh', 'thÃ nh phá»‘', 'xÃ£', 'thá»‹ tráº¥n', 'Ä‘Æ°á»ng', 'phá»‘']
        address_elements = soup.find_all(['p', 'div', 'span', 'td'])
        
        for elem in address_elements:
            text = elem.get_text(strip=True)
            if text and len(text) > 20:
                if any(keyword in text.lower() for keyword in address_keywords):
                    result["address"] = text
                    logger.info(f"ğŸ  Found address: {text}")
                    break
        
        # Method 5: Look for business information
        business_keywords = ['cÃ´ng ty', 'doanh nghiá»‡p', 'tá»• chá»©c', 'cÃ¡ nhÃ¢n', 'há»™ kinh doanh']
        for elem in soup.find_all(['p', 'div', 'span', 'td']):
            text = elem.get_text(strip=True)
            if text and any(keyword in text.lower() for keyword in business_keywords):
                result["business_type"] = text
                logger.info(f"ğŸ¢ Found business type: {text}")
                break
        
        # Method 6: Look for status information
        status_keywords = ['hoáº¡t Ä‘á»™ng', 'ngá»«ng hoáº¡t Ä‘á»™ng', 'Ä‘ang hoáº¡t Ä‘á»™ng', 'táº¡m nghá»‰']
        for elem in soup.find_all(['p', 'div', 'span', 'td']):
            text = elem.get_text(strip=True)
            if text and any(keyword in text.lower() for keyword in status_keywords):
                result["business_status"] = text
                logger.info(f"ğŸ“Š Found business status: {text}")
                break
        
        # Method 7: Look for dates
        date_pattern = r'\d{1,2}[/-]\d{1,2}[/-]\d{4}|\d{4}[/-]\d{1,2}[/-]\d{1,2}'
        date_matches = re.findall(date_pattern, all_text)
        if date_matches:
            result["registration_date"] = date_matches[0]
            logger.info(f"ğŸ“… Found date: {date_matches[0]}")
        
        # Method 8: Extract additional structured data
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    key = cells[0].get_text(strip=True)
                    value = cells[1].get_text(strip=True)
                    if key and value:
                        result["additional_info"][key] = value
        
        # Method 9: Look for specific data in divs with classes
        data_divs = soup.find_all('div', class_=re.compile(r'info|data|detail|profile'))
        for div in data_divs:
            text = div.get_text(strip=True)
            if text and len(text) > 5:
                result["additional_info"][f"div_content_{len(result['additional_info'])}"] = text
        
        logger.info(f"ğŸ“Š Extracted {len(result['additional_info'])} additional data fields")
        
        return result

def main():
    """Main function to test CCCD 031089011929"""
    cccd = "031089011929"
    
    logger.info("ğŸš€ Starting CCCD check for Module 2")
    logger.info(f"Target CCCD: {cccd}")
    logger.info("="*60)
    
    checker = CCCDChecker()
    result = checker.check_cccd(cccd)
    
    # Display results
    logger.info("\n" + "="*60)
    logger.info("ğŸ“Š Káº¾T QUáº¢ KIá»‚M TRA CCCD")
    logger.info("="*60)
    
    logger.info(f"CCCD: {result['cccd']}")
    logger.info(f"Tráº¡ng thÃ¡i: {result['status']}")
    
    if result['status'] == 'success':
        logger.info(f"â±ï¸ Thá»i gian pháº£n há»“i: {result.get('response_time', 0):.2f}s")
        
        if result.get('tax_code'):
            logger.info(f"ğŸ¯ MÃ£ sá»‘ thuáº¿: {result['tax_code']}")
        else:
            logger.info("ğŸ¯ MÃ£ sá»‘ thuáº¿: KhÃ´ng tÃ¬m tháº¥y")
        
        if result.get('name'):
            logger.info(f"ğŸ‘¤ TÃªn: {result['name']}")
        else:
            logger.info("ğŸ‘¤ TÃªn: KhÃ´ng tÃ¬m tháº¥y")
        
        if result.get('address'):
            logger.info(f"ğŸ  Äá»‹a chá»‰: {result['address']}")
        else:
            logger.info("ğŸ  Äá»‹a chá»‰: KhÃ´ng tÃ¬m tháº¥y")
        
        if result.get('business_type'):
            logger.info(f"ğŸ¢ Loáº¡i hÃ¬nh: {result['business_type']}")
        else:
            logger.info("ğŸ¢ Loáº¡i hÃ¬nh: KhÃ´ng tÃ¬m tháº¥y")
        
        if result.get('business_status'):
            logger.info(f"ğŸ“Š Tráº¡ng thÃ¡i: {result['business_status']}")
        else:
            logger.info("ğŸ“Š Tráº¡ng thÃ¡i: KhÃ´ng tÃ¬m tháº¥y")
        
        if result.get('registration_date'):
            logger.info(f"ğŸ“… NgÃ y Ä‘Äƒng kÃ½: {result['registration_date']}")
        else:
            logger.info("ğŸ“… NgÃ y Ä‘Äƒng kÃ½: KhÃ´ng tÃ¬m tháº¥y")
        
        if result.get('profile_url'):
            logger.info(f"ğŸ”— Link profile: {result['profile_url']}")
        else:
            logger.info("ğŸ”— Link profile: KhÃ´ng tÃ¬m tháº¥y")
        
        # Display additional information
        if result.get('additional_info'):
            logger.info(f"\nğŸ“‹ THÃ”NG TIN Bá»” SUNG ({len(result['additional_info'])} trÆ°á»ng):")
            for key, value in result['additional_info'].items():
                logger.info(f"  {key}: {value}")
        
        # Display all links found
        if result.get('all_links'):
            logger.info(f"\nğŸ”— Táº¤T Cáº¢ LINKS TÃŒM THáº¤Y ({len(result['all_links'])} links):")
            for i, link in enumerate(result['all_links'][:10], 1):  # Show first 10 links
                logger.info(f"  {i}. {link['text']} -> {link['url']}")
            if len(result['all_links']) > 10:
                logger.info(f"  ... vÃ  {len(result['all_links']) - 10} links khÃ¡c")
        
        # Save detailed results to file
        import json
        with open(f"cccd_{cccd}_detailed_results.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        logger.info(f"\nğŸ’¾ Káº¿t quáº£ chi tiáº¿t Ä‘Ã£ lÆ°u vÃ o: cccd_{cccd}_detailed_results.json")
        
    else:
        logger.error(f"âŒ Lá»—i: {result.get('error', 'Unknown error')}")
    
    logger.info("\nğŸ HoÃ n thÃ nh kiá»ƒm tra CCCD")

if __name__ == "__main__":
    main()